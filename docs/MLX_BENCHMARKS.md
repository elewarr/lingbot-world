# MLX Backend Benchmarks

> Auto-generated by `scripts/benchmark_mlx.py`
> Generated: 2026-02-05T21:51:00.310184

## Configuration

- **Hardware**: Apple Silicon (Apple M3 Ultra), 512GB
- **Model**: WanModel (synthetic, ~100M)
- **Resolution**: 480x832

## Results Summary

| Backend | Frames | Steps | Quant | Time/Step | Total | Model Mem | Speedup |
|---------|--------|-------|-------|-----------|-------|-----------|---------|
| PYTORCH | 5 | 5 | None | 0.0s | 0.0s | 95MB | 1.00x (baseline) |
| MLX | 5 | 5 | None | 0.0s | 0.0s | 95MB | 1.24x |
| PYTORCH | 41 | 5 | None | 0.1s | 0.4s | 95MB | 1.00x (baseline) |
| MLX | 41 | 5 | None | 0.0s | 0.1s | 95MB | 2.69x |

## Detailed Analysis

### Quantization Memory Savings

| Quantization | Avg Model Memory | Memory Reduction |
|--------------|------------------|------------------|
| None (FP32) | 95MB | - |

### Performance by Frame Count

**5 Frames:**

- PYTORCH (None): 0.0s/step, 0.0s total, 1.00x speedup
- MLX (None): 0.0s/step, 0.0s total, 1.24x speedup

**41 Frames:**

- PYTORCH (None): 0.1s/step, 0.4s total, 1.00x speedup
- MLX (None): 0.0s/step, 0.1s total, 2.69x speedup

## Methodology

### Benchmark Procedure

1. **Warmup**: 1-2 forward passes to trigger JIT compilation
2. **Timing**: Each denoising step is timed individually
3. **Synchronization**: GPU operations are synchronized before timing
4. **Memory**: Peak process memory is recorded

### Notes

- All timings exclude model loading and VAE encode/decode
- Memory figures are for a single DiT model (full pipeline uses 2x)
- Synthetic benchmarks use smaller models; full benchmarks use production weights
- MPS backend uses float32 (bfloat16 not supported)

## Running Benchmarks

```bash
# Quick synthetic benchmark (no model required)
python scripts/benchmark_mlx.py --mode synthetic

# Full benchmark with model weights
python scripts/benchmark_mlx.py --ckpt_dir models/lingbot-world-base-cam

# Specific configuration
python scripts/benchmark_mlx.py --frames 41 --steps 20 --quant nf4
```
