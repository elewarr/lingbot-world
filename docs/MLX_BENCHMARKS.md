# MLX Backend Benchmarks

> Auto-generated by `scripts/benchmark_mlx.py`
> Generated: 2026-02-05T18:38:18.358570

## Configuration

- **Hardware**: Apple Silicon (Apple M3 Ultra), 512GB
- **Model**: WanModel (synthetic, ~100M)
- **Resolution**: 480x832

## Results Summary

| Backend | Frames | Steps | Quant | Time/Step | Total | Model Mem | Speedup |
|---------|--------|-------|-------|-----------|-------|-----------|---------|
| PYTORCH | 5 | 5 | None | 0.0s | 0.0s | 95MB | 1.00x (baseline) |
| MLX | 5 | 5 | None | 0.0s | 0.0s | 95MB | 1.05x |
| MLX | 5 | 5 | int8 | 0.0s | 0.0s | 29MB | 1.05x |
| MLX | 5 | 5 | nf4 | 0.0s | 0.0s | 18MB | 1.03x |
| PYTORCH | 41 | 20 | None | 0.1s | 1.5s | 95MB | 1.00x (baseline) |
| MLX | 41 | 20 | None | 0.0s | 0.6s | 95MB | 2.49x |
| MLX | 41 | 20 | int8 | 0.0s | 0.6s | 29MB | 2.43x |
| MLX | 41 | 20 | nf4 | 0.0s | 0.6s | 18MB | 2.42x |

## Detailed Analysis

### Quantization Memory Savings

| Quantization | Avg Model Memory | Memory Reduction |
|--------------|------------------|------------------|
| None (FP32) | 95MB | - |
| int8 | 29MB | 69% |
| nf4 | 18MB | 81% |

### Performance by Frame Count

**5 Frames:**

- PYTORCH (None): 0.0s/step, 0.0s total, 1.00x speedup
- MLX (None): 0.0s/step, 0.0s total, 1.05x speedup
- MLX (int8): 0.0s/step, 0.0s total, 1.05x speedup
- MLX (nf4): 0.0s/step, 0.0s total, 1.03x speedup

**41 Frames:**

- PYTORCH (None): 0.1s/step, 1.5s total, 1.00x speedup
- MLX (None): 0.0s/step, 0.6s total, 2.49x speedup
- MLX (int8): 0.0s/step, 0.6s total, 2.43x speedup
- MLX (nf4): 0.0s/step, 0.6s total, 2.42x speedup

## Methodology

### Benchmark Procedure

1. **Warmup**: 1-2 forward passes to trigger JIT compilation
2. **Timing**: Each denoising step is timed individually
3. **Synchronization**: GPU operations are synchronized before timing
4. **Memory**: Peak process memory is recorded

### Notes

- All timings exclude model loading and VAE encode/decode
- Memory figures are for a single DiT model (full pipeline uses 2x)
- Synthetic benchmarks use smaller models; full benchmarks use production weights
- MPS backend uses float32 (bfloat16 not supported)

## Running Benchmarks

```bash
# Quick synthetic benchmark (no model required)
python scripts/benchmark_mlx.py --mode synthetic

# Full benchmark with model weights
python scripts/benchmark_mlx.py --ckpt_dir models/lingbot-world-base-cam

# Specific configuration
python scripts/benchmark_mlx.py --frames 41 --steps 20 --quant nf4
```
