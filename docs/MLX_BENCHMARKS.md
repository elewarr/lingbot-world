# MLX Backend Benchmarks

> Auto-generated by `scripts/benchmark_mlx.py`
> Generated: 2026-02-05T22:00:22.444100

## Configuration

- **Hardware**: Apple Silicon (Apple M3 Ultra), 512GB
- **Model**: WanModel (14B parameters)
- **Resolution**: 480x832

## Results Summary

| Backend | Frames | Steps | Quant | Time/Step | Total | Model Mem | Speedup |
|---------|--------|-------|-------|-----------|-------|-----------|---------|
| MLX | 41 | 20 | int8 | 7.1s | 2.4min | 19.5GB | 1.00x |
| MLX | 41 | 20 | nf4 | 6.9s | 2.3min | 10.8GB | 1.00x |

## Detailed Analysis

### Quantization Memory Savings

| Quantization | Avg Model Memory | Memory Reduction |
|--------------|------------------|------------------|
| int8 | 19.5GB | - |
| nf4 | 10.8GB | 44% |

### Performance by Frame Count

**41 Frames:**

- MLX (int8): 7.1s/step, 2.4min total, 1.00x speedup
- MLX (nf4): 6.9s/step, 2.3min total, 1.00x speedup

## Methodology

### RoPE Vectorization Optimization

The RoPE (Rotary Position Embeddings) implementation was optimized to use vectorized
batch operations instead of Python loops. Benchmark results (on Apple M3 Ultra):

| Configuration | Original | Vectorized | Speedup |
|---------------|----------|------------|---------|
| 480p (B=1, 4290 pos) | 1.01ms | 0.96ms | 1.05x |
| 720p (B=1, 32760 pos) | 7.2ms | 7.4ms | ~1.00x |
| Small (B=1, 1024 pos) | 0.48ms | 0.46ms | 1.04x |
| Batch (B=2, uniform) | 0.60ms | 0.51ms | 1.17x |
| Batch (B=4, uniform) | 0.91ms | 0.76ms | 1.20x |

Key improvements:
- Fast path for B=1 (most common in inference): 4-5% speedup
- Batch processing with uniform grids: 17-20% speedup
- Fully vectorized operations avoid breaking the MLX compute graph

Run: `PYTHONPATH=. python benchmarks/bench_rope_vectorized.py`

### Benchmark Procedure

1. **Warmup**: 1-2 forward passes to trigger JIT compilation
2. **Timing**: Each denoising step is timed individually
3. **Synchronization**: GPU operations are synchronized before timing
4. **Memory**: Peak process memory is recorded

### Notes

- All timings exclude model loading and VAE encode/decode
- Memory figures are for a single DiT model (full pipeline uses 2x)
- Synthetic benchmarks use smaller models; full benchmarks use production weights
- MPS backend uses float32 (bfloat16 not supported)

## Running Benchmarks

```bash
# Quick synthetic benchmark (no model required)
python scripts/benchmark_mlx.py --mode synthetic

# Full benchmark with model weights
python scripts/benchmark_mlx.py --ckpt_dir models/lingbot-world-base-cam

# Specific configuration
python scripts/benchmark_mlx.py --frames 41 --steps 20 --quant nf4
```
